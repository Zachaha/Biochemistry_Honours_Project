{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.uniprot.org/uniprotkb/A0A0R0ERV5/entry#names_and_taxonomy\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_code = response.text\n",
    "    print(html_code)\n",
    "else:\n",
    "    print(f\"Error: Unable to fetch the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a dataframe called af2 (564446 rows Ã— 73 columns) encoding the evaluation indexes of each corresponding alphafold2 predicted protein structures. For each row in the dataframe i want to use the string under index 'UniProt_ID' to fetch information from uniprot website f'https://www.uniprot.org/uniprotkb/{UniProt_ID}/entry#names_and_taxonomy'. By doing so, I can add a new column to the af2 dataframe called 'Domain' attached to each row and show the domain of the source of the protein (Eukaryota (eucaryotes),  Bacteria (eubacteria), Archaea, Viruses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "af2 = pd. read_csv('destress_data_af2.csv')\n",
    "af2.columns = ['design_name', 'file_name', 'full_sequence', 'dssp_assignment', 'composition_ALA', 'composition_CYS', \n",
    "               'composition_ASP', 'composition_GLU', 'composition_PHE', 'composition_GLY', 'composition_HIS', \n",
    "               'composition_ILE', 'composition_LYS', 'composition_LEU', 'composition_MET', 'composition_ASN', \n",
    "               'composition_PRO', 'composition_GLN', 'composition_ARG', 'composition_SER', 'composition_THR',\n",
    "                'composition_VAL', 'composition_TRP', 'composition_UNK', 'composition_TYR', 'ss_prop_alpha_helix', \n",
    "                'ss_prop_beta_bridge', 'ss_prop_beta_strand', 'ss_prop_3_10_helix', 'ss_prop_pi_helix', \n",
    "                'ss_prop_hbonded_turn', 'ss_prop_bend', 'ss_prop_loop', 'hydrophobic_fitness', 'isoelectric_point', \n",
    "                'charge', 'mass', 'num_residues', 'packing_density', 'budeff_total', 'budeff_steric', 'budeff_desolvation', \n",
    "                'budeff_charge', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total', \n",
    "                'evoef2_interD_total', 'dfire2_total', 'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', \n",
    "                'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', \n",
    "                'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', \n",
    "                'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close', \n",
    "                'rosetta_yhh_planarity', 'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', \n",
    "                'aggrescan3d_max_value']\n",
    "\n",
    "# Remove 'AF-' from the 'design_name' values\n",
    "af2[\"UniProt_ID\"] = af2[\"design_name\"].str.replace(\"AF-\", \"\")\n",
    "# Remove the last 12 characters from the 'UniProt_ID' values\n",
    "af2[\"UniProt_ID\"] = af2[\"UniProt_ID\"].str[:-12]\n",
    "#af2 = af2.drop_duplicates(subset='UniProt_ID', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "af2_random = af2.sample(n=10000, random_state=42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jq/ph87kw_d695bbwx2yc22bgxh0000gn/T/ipykernel_1304/3021281575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#af2_slice = pd.DataFrame(af2_random.iloc[0:1,:])  # Replace with 'af2' for the entire dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0muniprot_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maf2_random\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"UniProt_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniprot_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust max_workers as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0maf2_random\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Domain\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jq/ph87kw_d695bbwx2yc22bgxh0000gn/T/ipykernel_1304/3021281575.py\u001b[0m in \u001b[0;36mfetch_domains\u001b[0;34m(uniprot_ids, max_workers)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_domains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniprot_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_domain_from_uniprot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniprot_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_domain_from_uniprot(uniprot_id):\n",
    "    url = f\"https://www.uniprot.org/uniprot/{uniprot_id}.xml\"\n",
    "    response = requests.get(url, timeout=30)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "        lineage = soup.find(\"lineage\")\n",
    "        if lineage:\n",
    "            for taxon in lineage.find_all(\"taxon\"):\n",
    "                domain = taxon.get_text()\n",
    "                if domain in [\"Eukaryota\", \"Bacteria\", \"Archaea\", \"Viruses\"]:\n",
    "                    return domain\n",
    "    return None\n",
    "\n",
    "def fetch_domains(uniprot_ids, max_workers):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(get_domain_from_uniprot, uniprot_ids))\n",
    "    return results\n",
    "\n",
    "\n",
    "#af2_slice = pd.DataFrame(af2_random.iloc[0:1,:])  # Replace with 'af2' for the entire dataframe\n",
    "uniprot_ids = af2_random [\"UniProt_ID\"].tolist()\n",
    "domains = fetch_domains(uniprot_ids, max_workers=10)  # Adjust max_workers as needed\n",
    "af2_random[\"Domain\"] = domains\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named 'af2'\n",
    "#af2[\"Domain\"] = af2[\"UniProt_ID\"].apply(get_domain_from_uniprot)\n",
    "af2_random.to_csv('af2_random_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howlongittakes(timefor100,maxworkers):\n",
    "    totalhours = timefor100/100*100000/60/60\n",
    "    print (f'When Max Workers are {maxworkers}, it takes {totalhours}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_workers = 10, take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howlongittakes (3.9,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howlongittakes (10.6,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import re\n",
    "\n",
    "\n",
    "# Define a regular expression pattern for extracting the next link \n",
    "# from the response headers\n",
    "re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "\n",
    "# Configure the Retry object for handling retryable HTTP error statuses\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "\n",
    "# Set up a requests session for making requests with configured retries\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Function to extract the next link from the response headers\n",
    "def get_next_link(headers):\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "# Generator function to retrieve data in batches\n",
    "def get_batch(batch_url):\n",
    "    while batch_url:\n",
    "        response = session.get(batch_url)\n",
    "        response.raise_for_status()\n",
    "        total = response.headers[\"x-total-results\"]\n",
    "        yield response, total\n",
    "        batch_url = get_next_link(response.headers)\n",
    "\n",
    "# Function to build the UniProt query string from a list of UniProt accession IDs\n",
    "def build_taxonomy_query(uniprot_ids):\n",
    "    query = \" OR \".join(f\"accession:{accession}\" for accession in uniprot_ids)\n",
    "    return query\n",
    "\n",
    "# Set the batch size for UniProt queries\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize an empty dictionary to store fetched taxonomy data\n",
    "taxonomy_data = {}\n",
    "\n",
    "# Iterate through the UniProt IDs in the af2 DataFrame in batches\n",
    "for i in range(0, 10, batch_size):\n",
    "    # Get the current batch of UniProt IDs\n",
    "    uniprot_ids = af2[\"UniProt_ID\"].iloc[i:i+batch_size].tolist()\n",
    "\n",
    "    # Build the UniProt query string for the current batch of UniProt IDs\n",
    "    query = build_taxonomy_query(uniprot_ids)\n",
    "\n",
    "    # Construct the UniProt URL for fetching taxonomy information\n",
    "    url = f'https://rest.uniprot.org/uniprotkb/{query}.xml'\n",
    "\n",
    "    # Fetch taxonomy data for the current batch of UniProt IDs\n",
    "    for batch, total in get_batch(url):\n",
    "        # Process each line in the batch response, excluding the header line\n",
    "        for line in batch.text.splitlines()[1:]:\n",
    "            # Split the line into columns\n",
    "            accession, taxonomy_id, superkingdom, cls = line.split('\\t')\n",
    "\n",
    "            # Store the taxonomy data in the taxonomy_data dictionary\n",
    "            taxonomy_data[accession] = (taxonomy_id, superkingdom, cls)\n",
    "\n",
    "        # Print the progress\n",
    "        print(f'{len(taxonomy_data)} / {total}')\n",
    "\n",
    "# Map the fetched taxonomy data to the corresponding columns in the af2 DataFrame\n",
    "af2[\"Taxonomy_ID\"], af2[\"Superkingdom\"], af2[\"Class\"] = zip(*af2[\"UniProt_ID\"].map(taxonomy_data).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6cc605c2d7cc8109eba26885bc9c6f9d06908d04204db0a4b362c749a5decd76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
