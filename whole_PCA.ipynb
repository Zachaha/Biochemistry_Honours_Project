{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snsb\n",
    "#from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.patches import Circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = pd. read_csv('./core_database/destress_data_pdb.csv')\n",
    "pdb.columns = ['design_name', 'file_name', 'full_sequence', 'dssp_assignment', 'composition_ALA', 'composition_CYS', \n",
    "               'composition_ASP', 'composition_GLU', 'composition_PHE', 'composition_GLY', 'composition_HIS', \n",
    "               'composition_ILE', 'composition_LYS', 'composition_LEU', 'composition_MET', 'composition_ASN', \n",
    "               'composition_PRO', 'composition_GLN', 'composition_ARG', 'composition_SER', 'composition_THR',\n",
    "                'composition_VAL', 'composition_TRP', 'composition_UNK', 'composition_TYR', 'ss_prop_alpha_helix', \n",
    "                'ss_prop_beta_bridge', 'ss_prop_beta_strand', 'ss_prop_3_10_helix', 'ss_prop_pi_helix', \n",
    "                'ss_prop_hbonded_turn', 'ss_prop_bend', 'ss_prop_loop', 'hydrophobic_fitness', 'isoelectric_point', \n",
    "                'charge', 'mass', 'num_residues', 'packing_density', 'budeff_total', 'budeff_steric', 'budeff_desolvation', \n",
    "                'budeff_charge', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total', \n",
    "                'evoef2_interD_total', 'dfire2_total', 'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', \n",
    "                'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', \n",
    "                'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', \n",
    "                'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close', \n",
    "                'rosetta_yhh_planarity', 'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', \n",
    "                'aggrescan3d_max_value']\n",
    "\n",
    "af2 = pd. read_csv('./core_database/destress_data_af2.csv')\n",
    "af2.columns = ['design_name', 'file_name', 'full_sequence', 'dssp_assignment', 'composition_ALA', 'composition_CYS', \n",
    "               'composition_ASP', 'composition_GLU', 'composition_PHE', 'composition_GLY', 'composition_HIS', \n",
    "               'composition_ILE', 'composition_LYS', 'composition_LEU', 'composition_MET', 'composition_ASN', \n",
    "               'composition_PRO', 'composition_GLN', 'composition_ARG', 'composition_SER', 'composition_THR',\n",
    "                'composition_VAL', 'composition_TRP', 'composition_UNK', 'composition_TYR', 'ss_prop_alpha_helix', \n",
    "                'ss_prop_beta_bridge', 'ss_prop_beta_strand', 'ss_prop_3_10_helix', 'ss_prop_pi_helix', \n",
    "                'ss_prop_hbonded_turn', 'ss_prop_bend', 'ss_prop_loop', 'hydrophobic_fitness', 'isoelectric_point', \n",
    "                'charge', 'mass', 'num_residues', 'packing_density', 'budeff_total', 'budeff_steric', 'budeff_desolvation', \n",
    "                'budeff_charge', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total', \n",
    "                'evoef2_interD_total', 'dfire2_total', 'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', \n",
    "                'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', \n",
    "                'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', \n",
    "                'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close', \n",
    "                'rosetta_yhh_planarity', 'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', \n",
    "                'aggrescan3d_max_value']\n",
    "\n",
    "pisces = pd. read_csv('./core_database/cullpdb_pc95.0_res0.0-3.0_len40-10000_R0.3_Xray_d2023_02_13_chains47410.csv')\n",
    "pisces.columns = [\"PDBchain\", \"len\", \"method\", \"resol\", \"rfac\", \"freerfac\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop NaN and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = pdb.drop_duplicates(subset='full_sequence', keep='first')\n",
    "pdb.dropna(subset=['full_sequence'], inplace=True)\n",
    "\n",
    "pdb= pdb.drop(['dfire2_total','budeff_total', 'budeff_steric', 'budeff_desolvation', 'budeff_charge',\n",
    "               'composition_UNK','rosetta_yhh_planarity','evoef2_interD_total'],axis=1)\n",
    "\n",
    "pdb= pdb.dropna(how='any')\n",
    "pdb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "af2 = af2.drop_duplicates(subset='full_sequence', keep='first')\n",
    "af2.dropna(subset=['full_sequence',], inplace=True)\n",
    "\n",
    "af2= af2.drop(['dfire2_total','budeff_total', 'budeff_steric', 'budeff_desolvation', 'budeff_charge',\n",
    "               'composition_UNK','rosetta_yhh_planarity','evoef2_interD_total'],axis=1)\n",
    "\n",
    "af2= af2.dropna(how='any')\n",
    "af2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster the compositions into label form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cluster the data about the amino acid composition into subgroups\n",
    "pdb['composition_hydrophobic'] = pdb['composition_ALA'] + pdb['composition_VAL'] + pdb['composition_LEU'] + pdb['composition_ILE'] + pdb['composition_MET'] + pdb['composition_PHE'] + pdb['composition_TYR'] + pdb['composition_TRP']\n",
    "pdb['composition_polar'] = pdb['composition_ASN'] + pdb['composition_GLN'] + pdb['composition_SER'] + pdb['composition_THR']\n",
    "pdb['composition_acidic'] = pdb['composition_ASP'] + pdb['composition_GLU']\n",
    "pdb['composition_basic'] = pdb['composition_ARG'] + pdb['composition_HIS'] + pdb['composition_LYS']\n",
    "pdb['composition_cysteine'] = pdb['composition_CYS']\n",
    "pdb['composition_glycine'] = pdb['composition_GLY']\n",
    "pdb['composition_proline'] = pdb['composition_PRO']\n",
    "pdb.drop(columns=['composition_ALA', 'composition_CYS', 'composition_ASP', 'composition_GLU', \n",
    "                'composition_PHE', 'composition_GLY', 'composition_HIS', 'composition_ILE', \n",
    "                'composition_LYS', 'composition_LEU', 'composition_MET', 'composition_ASN', \n",
    "                'composition_PRO', 'composition_GLN', 'composition_ARG', 'composition_SER', \n",
    "                'composition_THR', 'composition_VAL', 'composition_TRP', 'composition_TYR'], inplace=True)\n",
    "\n",
    "af2['composition_hydrophobic'] = af2['composition_ALA'] + af2['composition_VAL'] + af2['composition_LEU'] + af2['composition_ILE'] + af2['composition_MET'] + af2['composition_PHE'] + af2['composition_TYR'] + af2['composition_TRP']\n",
    "af2['composition_polar'] = af2['composition_ASN'] + af2['composition_GLN'] + af2['composition_SER'] + af2['composition_THR']\n",
    "af2['composition_acidic'] = af2['composition_ASP'] + af2['composition_GLU']\n",
    "af2['composition_basic'] = af2['composition_ARG'] + af2['composition_HIS'] + af2['composition_LYS']\n",
    "af2['composition_cysteine'] = af2['composition_CYS']\n",
    "af2['composition_glycine'] = af2['composition_GLY']\n",
    "af2['composition_proline'] = af2['composition_PRO']\n",
    "af2.drop(columns=['composition_ALA', 'composition_CYS', 'composition_ASP', 'composition_GLU', \n",
    "                'composition_PHE', 'composition_GLY', 'composition_HIS', 'composition_ILE', \n",
    "                'composition_LYS', 'composition_LEU', 'composition_MET', 'composition_ASN', \n",
    "                'composition_PRO', 'composition_GLN', 'composition_ARG', 'composition_SER', \n",
    "                'composition_THR', 'composition_VAL', 'composition_TRP', 'composition_TYR'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create labels for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a list of the culling protein id PDBchain_list\n",
    "PDBchain = pisces['PDBchain'].str.extract(r'(\\w{4})').squeeze()\n",
    "PDBchain_list = PDBchain.tolist()\n",
    "\n",
    "pdb['Entry'] = pdb['design_name'].str[-4:].str.upper()\n",
    "af2['Entry'] = af2['design_name'].apply(lambda x: x.split('-')[1])\n",
    "\n",
    "pdb['label_1']='PDB'\n",
    "af2['label_1']='AF2'\n",
    "af2['label_2']=''\n",
    "pdb['label_2']=''\n",
    "\n",
    "mask = pdb['Entry'].isin(PDBchain_list)\n",
    "\n",
    "# Assign the value 'Pisces' to the 'label' column for rows where the mask is True\n",
    "pdb.loc[mask, 'label_2'] = 'Pisces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the AF2 cath_data\n",
    "cath_data_af2 = pd.read_csv ('/Users/Zachary/Desktop/Dissertation/CATH/cath-v4_3_0.alphafold-v2.2022-11-22.tsv',sep='\\t+',index_col=0)\n",
    "cath_data_af2.drop(columns = ['source','length','class','assignment',\n",
    "                            'pLDDT','LUR','SSEs','perc_not_in_SS',\n",
    "                            'packing'],inplace = True)\n",
    "\n",
    "cath_data_af2.reset_index(inplace=True)\n",
    "cath_data_af2['ID'] = cath_data_af2['domain_ID'].apply(lambda x: x.split('_')[1])\n",
    "cath_data_af2.drop(columns= ['domain_ID'],inplace=True)\n",
    "\n",
    "#Prepare the PDB CATH_data\n",
    "# Read the text file into a pandas DataFrame\n",
    "cath_data_pdb = pd.read_csv('/Users/Zachary/Desktop/Dissertation/CATH/CATH_daily_snapshot/cath-b-newest-all.txt', sep=' ',header = 0)\n",
    "# Assign column names to the DataFrame\n",
    "cath_data_pdb.columns = [\"CATH domain name\", 'version',\"CATH num\",'frag:chain']\n",
    "cath_data_pdb.drop(columns= ['version','frag:chain'], inplace =True)\n",
    "cath_data_pdb['ID'] = cath_data_pdb['CATH domain name'].str[:-3]\n",
    "\n",
    "cathnum_unique = ~cath_data_pdb.copy().duplicated(subset = 'CATH num',keep = False)\n",
    "unique_cath = cath_data_pdb[cathnum_unique]\n",
    "filtered_cath = unique_cath.drop_duplicates(subset = 'ID',keep = 'first')\n",
    "\n",
    "#将filter_cath从cath中删除后，剩下的行即便有着重复的PDBID，但他们的PDB ID也相同 (去掉了2462行)\n",
    "filtered_pdbid = filtered_cath['ID'].unique()\n",
    "rows_to_keep = ~cath_data_pdb['ID'].isin(filtered_pdbid)\n",
    "cath_data_pdb = cath_data_pdb[rows_to_keep]\n",
    "\n",
    "#在此之后，我们就可以去掉重复的PDB ID，将两个dataframe合并 （现在有139436行）\n",
    "cath_data_pdb.drop_duplicates(subset = 'ID',keep = 'first',inplace = True)\n",
    "cath_data_pdb.reset_index(drop = True, inplace = True)\n",
    "cath_data_pdb.drop(columns= ['CATH domain name'], inplace =True)\n",
    "cath_data_pdb['ID'] = cath_data_pdb['ID'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge wholedata_name with cath_data_af2\n",
    "pdb['label_3']=''\n",
    "af2['label_3']=''\n",
    "\n",
    "cath_data_af2_unique = cath_data_af2.drop_duplicates(subset='ID')\n",
    "cath_data_pdb_unique = cath_data_pdb.drop_duplicates(subset='ID')\n",
    "\n",
    "af2 = af2.merge(cath_data_af2_unique[['ID', 'sfam_id']],\n",
    "                                                 left_on='Entry', right_on='ID', how='left')\n",
    "\n",
    "pdb =  pdb.merge(cath_data_pdb_unique[['ID', 'CATH num']],\n",
    "                                left_on='Entry', right_on='ID', how='left' )\n",
    "\n",
    "                            \n",
    "af2['label_3']=af2['sfam_id']\n",
    "pdb['label_3']=pdb['CATH num']\n",
    "\n",
    "af2.drop (columns = ['ID','sfam_id'], inplace = True)\n",
    "\n",
    "pdb.drop(columns=['ID','CATH num'],inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normailization by dividing the energy functions by number of residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the value of energy function by dividing the number of residue\n",
    "pdb.loc[:,[\n",
    "    'hydrophobic_fitness', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total', 'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', 'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', 'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', 'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close',  'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', 'aggrescan3d_max_value',\n",
    "],] = pdb.loc[:,[\n",
    "    'hydrophobic_fitness', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total', 'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', 'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', 'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', 'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close',  'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', 'aggrescan3d_max_value',\n",
    "],].div(pdb['num_residues'],axis=0) \n",
    "\n",
    "af2.loc[:,[\n",
    "    'hydrophobic_fitness', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total', 'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', 'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', 'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', 'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close', 'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', 'aggrescan3d_max_value',\n",
    "],] = af2.loc[:,[\n",
    "    'hydrophobic_fitness', 'evoef2_total', 'evoef2_ref_total', 'evoef2_intraR_total', 'evoef2_interS_total',  'rosetta_total', 'rosetta_fa_atr', 'rosetta_fa_rep', 'rosetta_fa_intra_rep', 'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd', 'rosetta_fa_intra_sol_xover4', 'rosetta_hbond_lr_bb', 'rosetta_hbond_sr_bb', 'rosetta_hbond_bb_sc', 'rosetta_hbond_sc', 'rosetta_dslf_fa13', 'rosetta_rama_prepro', 'rosetta_p_aa_pp', 'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close',  'aggrescan3d_total_value', 'aggrescan3d_avg_value', 'aggrescan3d_min_value', 'aggrescan3d_max_value',\n",
    "],].div(af2['num_residues'],axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop weird distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove composition and secondary structures with weird distribution\n",
    "pdb.drop(['ss_prop_alpha_helix', 'ss_prop_beta_bridge', 'ss_prop_beta_strand', 'ss_prop_3_10_helix',\n",
    "           'ss_prop_pi_helix'],axis = 1, inplace = True)\n",
    "af2.drop(['ss_prop_alpha_helix', 'ss_prop_beta_bridge', 'ss_prop_beta_strand', 'ss_prop_3_10_helix',\n",
    "           'ss_prop_pi_helix'],axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make numeric dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata = pd.concat([pdb, af2], axis=0)#0-130665 rows - PDB #130666 - 692709 rows - AF2\n",
    "wholedata.reset_index(drop=True, inplace=True)\n",
    "wholedata.to_csv ('./core_database/destress_wholedata_labelled_as_rd.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRELATION starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata = pd.read_csv('./core_database/destress_wholedata_labelled_as_rd.csv')\n",
    "wholedata.drop([\n",
    "    'evoef2_ref_total','rosetta_dslf_fa13', #too many zeros\n",
    "    'aggrescan3d_total_value' #have a averaged alternative\n",
    "                ], axis=1, inplace=True)\n",
    "#isolate the numeric data from dataframe\n",
    "wholedata_num = wholedata.select_dtypes(include=[pd.np.number])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "wholedata_normalize = pd.DataFrame(scaler.fit_transform(wholedata_num), columns=wholedata_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate both Pearson's r and Spearman's rho for wholedata_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata_normalized = wholedata_normalize.copy()\n",
    "wholedata_normalized = pd.DataFrame(scaler.fit_transform(wholedata_num), columns=wholedata_num.columns)\n",
    "wholedata_normalized.drop([\n",
    "    'evoef2_ref_total','rosetta_dslf_fa13', #too many zeros\n",
    "    'aggrescan3d_total_value', #have a averaged alternative\n",
    "    'mass','num_residues','charge',#highly correlated\n",
    "    'evoef2_total','evoef2_intraR_total','evoef2_interS_total',\n",
    "    'rosetta_total','rosetta_fa_atr','rosetta_fa_elec','rosetta_fa_sol','rosetta_rama_prepro','rosetta_omega',\n",
    "    'rosetta_p_aa_pp','rosetta_fa_dun','rosetta_hbond_bb_sc',\n",
    "    'packing_density',\n",
    "    'aggrescan3d_min_value'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson's correlation\n",
    "#pearson_corr = wholedata_normalized.corr(method='pearson')\n",
    "#pearson_corr.to_csv('./output_tables/Correlation/wholedata__pearson_correlation_matrix.csv')\n",
    "\n",
    "# Calculate Spearman's correlation\n",
    "spearman_corr = wholedata_normalize.corr(method='spearman')\n",
    "#spearman_corr.to_csv('./output_tables/Correlation/wholedata__spearman_correlation_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out highly correlated indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correlation threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# Find indexes with high Pearson correlation\n",
    "#high_pearson_corr = np.where(np.abs(pearson_corr) > threshold)\n",
    "#pearson_pairs = [(pearson_corr.index[x], pearson_corr.columns[y]) for x, y in zip(*high_pearson_corr) if x != y]\n",
    "\n",
    "# Find indexes with high Spearman correlation\n",
    "high_spearman_corr = np.where(np.abs(spearman_corr) > threshold)\n",
    "spearman_pairs = [(spearman_corr.index[x], spearman_corr.columns[y]) for x, y in zip(*high_spearman_corr) if x != y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "#print(\"Highly correlated pairs (Pearson):\")\n",
    "#for pair in pearson_pairs:\n",
    " #   print(pair, pearson_corr.loc[pair])\n",
    "\n",
    "print(\"\\nHighly correlated pairs (Spearman):\")\n",
    "for pair in spearman_pairs:\n",
    "    print(pair, spearman_corr.loc[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.01\n",
    "spearman_corr_high = spearman_corr.copy()\n",
    "spearman_corr_high[np.abs(spearman_corr_high) < threshold] = 0\n",
    "np.fill_diagonal(spearman_corr_high.values, 0)\n",
    "spearman_corr_high = spearman_corr_high.loc[(spearman_corr_high != 0).any(axis=1), (spearman_corr_high != 0).any(axis=0)]\n",
    "\n",
    "# Create a mask to show only the upper half of the heatmap\n",
    "#mask = np.zeros_like(spearman_corr_high)\n",
    "#mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "mask_upper_triangle = np.triu(np.ones_like(spearman_corr_high, dtype=bool), k=1)\n",
    "\n",
    "annot_values = spearman_corr_high.where(~mask_upper_triangle).applymap(lambda x: '{:.2f}'.format(x) if x != 0 else '')\n",
    "#annot_values = spearman_corr_high.applymap(lambda x: '{:.2f}'.format(x) if x != 0 else '')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "snsb.heatmap(spearman_corr_high, annot=annot_values, cmap='coolwarm', vmin=-1, vmax=1, mask=mask_upper_triangle,fmt='',annot_kws={\"fontsize\": 8})\n",
    "\n",
    "plt.title(\"Spearman Correlation Heatmap\", fontsize=30)\n",
    "plt.xticks(fontsize=17)\n",
    "plt.yticks(fontsize=17)\n",
    "# Adjust the position of the heatmap within the figure\n",
    "bottom_margin = 0.18  # Change this value to adjust the bottom margin\n",
    "top_margin = 0.93 # Change this value to adjust the top margin\n",
    "left_margin = 0.18  # Change this value to adjust the left margin\n",
    "right_margin =0.93   # Change this value to adjust the right margin\n",
    "plt.subplots_adjust(bottom=bottom_margin, top=top_margin, left=left_margin, right=right_margin)\n",
    "\n",
    "#plt.savefig('./figures/correlation_figure_spearman_half', dpi=400)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize the count dictionaries\n",
    "pearson_pairs_count = defaultdict(int)\n",
    "spearman_pairs_count = defaultdict(int)\n",
    "\n",
    "\n",
    "# Count the frequency of each index in overlapping_pairs\n",
    "for pair in pearson_pairs:\n",
    "    pearson_pairs_count[pair[0]] += 1\n",
    "    pearson_pairs_count[pair[1]] += 1\n",
    "\n",
    "# Count the frequency of each index in unique_pearson_pairs\n",
    "for pair in spearman_pairs:\n",
    "    spearman_pairs_count[pair[0]] += 1\n",
    "    spearman_pairs_count[pair[1]] += 1\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Index frequency in pearson pairs:\")\n",
    "for index, count in pearson_pairs_count.items():\n",
    "    print(f\"{index}: {count}\")\n",
    "\n",
    "print(\"\\nIndex frequency in spearman pairs:\")\n",
    "for index, count in spearman_pairs_count.items():\n",
    "    print(f\"{index}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the lists of pairs to sets\n",
    "pearson_pairs_set = set(pearson_pairs)\n",
    "spearman_pairs_set = set(spearman_pairs)\n",
    "\n",
    "# Find overlapping pairs\n",
    "overlapping_pairs = pearson_pairs_set.intersection(spearman_pairs_set)\n",
    "print(\"Overlapping pairs:\")\n",
    "for pair in overlapping_pairs:\n",
    "    print(pair)\n",
    "\n",
    "# Find pairs present only in Pearson's correlation results\n",
    "unique_pearson_pairs = pearson_pairs_set.difference(spearman_pairs_set)\n",
    "print(\"\\nUnique pairs in Pearson's correlation:\")\n",
    "for pair in unique_pearson_pairs:\n",
    "    print(pair)\n",
    "\n",
    "# Find pairs present only in Spearman's correlation results\n",
    "unique_spearman_pairs = spearman_pairs_set.difference(pearson_pairs_set)\n",
    "print(\"\\nUnique pairs in Spearman's correlation:\")\n",
    "for pair in unique_spearman_pairs:\n",
    "    print(pair)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove sequence properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wholedata_rd =  wholedata_normalized[[\n",
    "'ss_prop_hbonded_turn',\n",
    " 'ss_prop_bend',\n",
    " 'ss_prop_loop',\n",
    " 'hydrophobic_fitness',\n",
    " 'isoelectric_point',\n",
    " 'rosetta_fa_rep',\n",
    " 'rosetta_fa_intra_rep',\n",
    " 'rosetta_lk_ball_wtd',\n",
    " 'rosetta_fa_intra_sol_xover4',\n",
    " 'rosetta_hbond_lr_bb',\n",
    " 'rosetta_hbond_sr_bb',\n",
    " 'rosetta_hbond_sc',\n",
    " 'rosetta_pro_close',\n",
    " 'aggrescan3d_avg_value',\n",
    " 'aggrescan3d_max_value',]]\n",
    "wholedata_rd.to_csv ('./core_database/prepared_dimension_reduction_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata = pd.read_csv('./core_database/destress_wholedata_labelled_as_rd.csv')\n",
    "wholedata.columns=wholedata.columns.to_list()\n",
    "\n",
    "features = [\n",
    " 'ss_prop_hbonded_turn',\n",
    " 'ss_prop_bend',\n",
    " 'ss_prop_loop',\n",
    " 'hydrophobic_fitness',\n",
    " 'isoelectric_point',\n",
    " 'rosetta_fa_rep',\n",
    " 'rosetta_fa_intra_rep',\n",
    " 'rosetta_lk_ball_wtd',\n",
    " 'rosetta_fa_intra_sol_xover4',\n",
    " 'rosetta_hbond_lr_bb',\n",
    " 'rosetta_hbond_sr_bb',\n",
    " 'rosetta_hbond_sc',\n",
    " 'rosetta_pro_close',\n",
    " 'aggrescan3d_avg_value',\n",
    " 'aggrescan3d_max_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 1 PCA\n",
    "# Separating out the \n",
    "x_1 = wholedata.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y_1 = wholedata.loc[:,['Entry', 'label_1', 'label_2','composition_hydrophobic', 'composition_polar', 'composition_acidic','composition_basic', 'composition_cysteine', 'composition_glycine','composition_proline']].values\n",
    "\n",
    "# Standardizing the features\n",
    "x_1 = MinMaxScaler().fit_transform(x_1)\n",
    "pca_2d_1 = PCA(n_components=2)\n",
    "\n",
    "principalComponents_1 = pca_2d_1.fit_transform(x_1)\n",
    "\n",
    "principalDf_2d_1 = pd.DataFrame(data = principalComponents_1\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata.loc[:,['label_1','label_2']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "#finalDf_2d_1 =finalDf_2d_1.sample(n = 10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the PCA data from your previous code\n",
    "af2_data = finalDf_2d_1[(finalDf_2d_1['label_1'] == 'AF2') | (finalDf_2d_1['label_2'] == 'AF2')]\n",
    "pdb_data = finalDf_2d_1[(finalDf_2d_1['label_1'] == 'PDB') | (finalDf_2d_1['label_2'] == 'PDB')]\n",
    "pisces_data = finalDf_2d_1[(finalDf_2d_1['label_1'] == 'Pisces') | (finalDf_2d_1['label_2'] == 'Pisces')]\n",
    "\n",
    "x_af2 = af2_data['principal component 1'].values\n",
    "y_af2 = af2_data['principal component 2'].values\n",
    "x_pdb = pdb_data['principal component 1'].values\n",
    "y_pdb = pdb_data['principal component 2'].values\n",
    "x_pisces = pisces_data['principal component 1'].values\n",
    "y_pisces = pisces_data['principal component 2'].values\n",
    "\n",
    "\n",
    "def scatter_hist(x, y, ax, ax_histx, ax_histy, color):\n",
    "    ax.tick_params(axis='x', which='major', labelsize=20)  # Change 20 to your desired font size for x-axis ticks\n",
    "    ax.tick_params(axis='y', which='major', labelsize=20)  # Change 20 to your desired font size for y-axis ticks\n",
    "   # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y, c=color, alpha=0.5)\n",
    "    \n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.25\n",
    "\n",
    "    bins_x = np.linspace(min(x), max(x), 200)\n",
    "    bins_y = np.linspace(min(y), max(y),200)\n",
    "\n",
    "    ax_histx.tick_params(axis='x', which='major', labelbottom=False)\n",
    "    ax_histx.tick_params(axis='y', which='major',labelsize=20)\n",
    "    \n",
    "    ax_histy.tick_params(axis='y', which='major', labelleft=False)\n",
    "    ax_histy.tick_params(axis='x', which='major', labelsize=20,rotation =270)\n",
    "    \n",
    "    ax_histx.hist(x, bins=bins_x, color=color, alpha=0.7, edgecolor='none')\n",
    "    ax_histy.hist(y, bins=bins_y, orientation='horizontal', color=color, alpha=0.7, edgecolor='none')\n",
    "\n",
    "\n",
    "def add_loading_vectors(ax, loadings, labels, scale=1, fontsize=25, color='r', threshold=0.15, label_offset_ratio=0.07):\n",
    "    for i, (x, y) in enumerate(loadings.T):\n",
    "        if abs(x) >= threshold or abs(y) >= threshold:\n",
    "            ax.arrow(0, 0, x * scale, y * scale, color=color, alpha=0.5)\n",
    "            # Calculate the label position based on the direction of the loading vector\n",
    "            label_x = x * scale * (1 + label_offset_ratio)\n",
    "            label_y = y * scale * (1 + label_offset_ratio)\n",
    "            ax.text(label_x, label_y, labels[i], color=color, fontsize=fontsize, ha='center', va='center')\n",
    "\n",
    "loadings = pca_2d_1.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a square Figure.\n",
    "fig = plt.figure(figsize=(25,25))\n",
    "\n",
    "# Add a gridspec with two rows and two columns and a ratio of 1 to 4 between\n",
    "# the size of the marginal axes and the main axes in both directions.\n",
    "# Also adjust the subplot parameters for a square plot.\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(5, 1), height_ratios=(1, 5),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.05, hspace=0.05)\n",
    "\n",
    "# Create the Axes.\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "xticks = np.arange(-1, 1.2, 0.2)\n",
    "yticks = np.arange(-1, 1.2, 0.2)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "targets = [ 'AF2','PDB','Pisces',]\n",
    "#targets = [ 'PDB','AF2']\n",
    "\n",
    "colors = [ '#D5D6BF','#B87C1B', '#6279A2',]\n",
    "#colors = ['#B87C1B', '#D5D6BF']\n",
    "\n",
    "# Set tick label size for the scatter plot\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "# Draw the scatter plot and marginals.\n",
    "scatter_hist(x_af2, y_af2, ax, ax_histx, ax_histy, color='#D5D6BF')\n",
    "scatter_hist(x_af2, y_af2, ax, ax_histx, ax_histy, color='#D5D6BF')\n",
    "scatter_hist(x_pdb, y_pdb, ax, ax_histx, ax_histy, color='#B87C1B')\n",
    "scatter_hist(x_pisces, y_pisces, ax, ax_histx, ax_histy, color='#6279A2')\n",
    "\n",
    "# Add the loading vectors to the plot\n",
    "labels = features # Replace with your original variable names\n",
    "add_loading_vectors(ax, loadings, labels, fontsize=25, color='red')\n",
    "\n",
    "# Set axis labels\n",
    "\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 30)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 30)\n",
    "fig.suptitle('2 component PCA of DE-STRESS database', fontsize = 50, y=0.93)\n",
    "\n",
    "# Create legend\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target),\n",
    "                        linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "ax.legend(handles=legend_handles, prop={'size': 45})\n",
    "plt.savefig('./figures/dimension_reduction/PCA_.png',dpi=300)\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize the PCA result on scatter plot\n",
    "\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_title('2 component PCA of DE-STRESS database', fontsize = 35)\n",
    "ax.set_xticks(np.arange(-1, 1, 0.1))\n",
    "ax.set_yticks(np.arange(-1, 1, 0.1))\n",
    "\n",
    "targets = [ 'AF2','PDB','Pisces',]\n",
    "#targets = [ 'PDB','AF2']\n",
    "\n",
    "colors = [ '#D5D6BF','#B87C1B', '#6279A2',]\n",
    "#colors = ['#B87C1B', '#D5D6BF']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['label_1'] == target) | (finalDf_2d_1['label_2'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "    \n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target),\n",
    "                        linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "# Add the custom legend to the plot\n",
    "\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "regions = [\n",
    "    \n",
    "    {'center': (0, 0.7), 'radius':  0.05,'label': 1},\n",
    "    {'center': (0, -0.17), 'radius':  0.05,'label': 3},\n",
    "    {'center': (0, -0.55), 'radius':  0.05,'label': 4},\n",
    "    {'center': (-0.55, 0), 'radius':  0.05,'label': 2},\n",
    "    {'center': (0.1, 0), 'radius':  0.05,'label': 5},\n",
    "    {'center': (0.4, 0), 'radius':  0.05,   'label': 6},\n",
    "    {'center': (0.72, 0), 'radius':  0.05,   'label': 7},\n",
    "]\n",
    "for region in regions:\n",
    "    circle = Circle(region['center'], region['radius'], edgecolor='red',\n",
    "                    facecolor='none', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    label_x = region['center'][0] + region['radius'] + 0.01  # Adjust the '5' for the desired offset\n",
    "    label_y = region['center'][1]\n",
    "    \n",
    "    # For label '4', adjust the y position to be above the circle\n",
    "    if region['label'] == 2:\n",
    "        label_y += region['radius'] + 0.02\n",
    "    \n",
    "    # Increase fontsize and set weight to 'bold' for thicker text\n",
    "    ax.annotate(str(region['label']), (label_x, label_y), fontsize=40, weight='bold', ha='left', va='center')\n",
    "ax.grid()\n",
    "# Display the plot\n",
    "plt.savefig('./figures/dimension_reduction/PCA_for_region_labeling.png',dpi=300)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_color):\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "hex_color = '#6279A2' #[ '#D5D6BF','#B87C1B', '#6279A2',]\n",
    "\n",
    "rgb_color = hex_to_rgb(hex_color)\n",
    "print(rgb_color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 1 PCA: To find out the ratio of explained variance by each components\n",
    "#PCA result analysis\n",
    "explained_variance_ratio = pca_2d_1.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Interpret principal components\n",
    "loadings = pca_2d_1.components_\n",
    "loadings\n",
    "loading_matrix = pd.DataFrame(loadings, columns=features)\n",
    "#print(\"Loading Matrix:\\n\", loading_matrix)\n",
    "loading_matrix.to_csv('./output_tables/2dPCA_1_principal_components_interpretation_1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier finder function\n",
    "def find_extreme_outliers(df, col1, col2):\n",
    "    # Calculate IQR for both columns\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define Boolean masks for extreme outliers in each column\n",
    "    mask1 = (df[col1] < Q1[col1] - 1.5 * IQR[col1]) | (df[col1] > Q3[col1] + 1.5 * IQR[col1])\n",
    "    mask2 = (df[col2] < Q1[col2] - 1.5 * IQR[col2]) | (df[col2] > Q3[col2] + 1.5 * IQR[col2])\n",
    "    \n",
    "    # Combine masks to find rows with extreme outliers in either column\n",
    "    outliers = df[(mask1 | mask2)]\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = find_extreme_outliers(principalDf_2d_1, \"principal component 1\", \"principal component 2\")\n",
    "outlier_list = outliers.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (25,25))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_title('2 component PCA of DE-STRESS database', fontsize = 35)\n",
    "\n",
    "targets = [ 'AF2','PDB','Pisces','Outliers']\n",
    "#targets = [ 'PDB','AF2']\n",
    "\n",
    "colors = [ '#D5D6BF','#B87C1B', '#6279A2','#E04C5E']\n",
    "#colors = ['#B87C1B', '#D5D6BF']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['label_1'] == target) | (finalDf_2d_1['label_2'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "    # Outliers\n",
    "    outlier_indices = indicesToKeep & finalDf_2d_1.index.isin(outlier_list)\n",
    "    ax.scatter(finalDf_2d_1.loc[outlier_indices, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[outlier_indices, 'principal component 2']\n",
    "               , c = '#E04C5E'\n",
    "               , s = 50\n",
    "               , alpha=0.5\n",
    "               )  # Square marker\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target), linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "# Add the custom legend to the plot\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find retions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_entry_values(outliers, wholedata):\n",
    "    # Get the index values of the outliers\n",
    "    outlier_indices = outliers.index\n",
    "    \n",
    "    # Retrieve the corresponding Entry values from the wholedata DataFrame\n",
    "    entry_values = wholedata.loc[outlier_indices, 'Entry']\n",
    "    \n",
    "    # Return the Entry values as a DataFrame\n",
    "    return entry_values.to_frame()\n",
    "\n",
    "#find interesting area in the plot \n",
    "def areafinder(df, min_1, max_1, min_2, max_2):\n",
    "    list_of_structures = []\n",
    "    for index, row in df.iterrows():\n",
    "        if min_1 < row[0] < max_1 and min_2 < row[1] < max_2:\n",
    "            list_of_structures.append(index)\n",
    "\n",
    "    list_of_uniprot_ids = []\n",
    "    for item in list_of_structures:\n",
    "        uniprot_id = uniprotidfinder(item)\n",
    "        list_of_uniprot_ids.append(uniprot_id)\n",
    "\n",
    "    return list_of_uniprot_ids\n",
    "def uniprotidfinder(index):\n",
    "    return wholedata.loc[index, ['Entry','label_2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region1 = areafinder(finalDf_2d_1, -0.05, 0.05, 0.65, 0.75)\n",
    "region2 = areafinder(finalDf_2d_1, -0.05, 0.05, -0.22, -0.12)\n",
    "region3 = areafinder(finalDf_2d_1, -0.05, 0.05, -0.6, -0.5)\n",
    "region4 = areafinder(finalDf_2d_1, -0.6, -0.5, -0.05, 0.05)\n",
    "region5 = areafinder(finalDf_2d_1, 0.05, 0.15, -0.05, 0.05)\n",
    "region6 = areafinder(finalDf_2d_1, 0.35, 0.45, -0.05, 0.05)\n",
    "region7 = areafinder(finalDf_2d_1, 0.67, 0.77, -0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_samples = 100\n",
    "random_elements = random.sample(region5, num_samples)\n",
    "\n",
    "print(random_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_value = \"6B87\"\n",
    "selected_rows = entry_values_outliers.loc[entry_values_outliers['Entry'] == entry_value]\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf_2d_1.iloc[1067]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wholedata['composition_glycine'].describe()\n",
    "#'charge', 'mass','number of residues','composition_hydrophobic','composition_polar','composition_acidic', \n",
    "#'composition_basic','composition_cysteine','composition_glycine', 'composition_proline','label_3'\n",
    "\n",
    "\n",
    "#'isoelectric_point','mass',\n",
    "# 'composition_hydrophobic',\n",
    "# 'composition_acidic','composition_basic','composition_cysteine','composition_glycine',\n",
    "# 'composition_proline','label_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " 'hydrophobic_fitness',\n",
    " 'packing_density',\n",
    " 'composition_hydrophobic',\n",
    " 'label_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata['packing_density'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by composition of glycine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 0.047,0.081, float('inf')]\n",
    "labels = ['less gly','moderate gly','more gly']\n",
    "wholedata_1=wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['composition_glycine'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "finalDf_2d_1 =finalDf_2d_1.sample(n = 100000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (35,35))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets = ['less gly','moderate gly','more gly']\n",
    "colors = [ '#D73027','#FFFFBF','#ABD9E9']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "ax.legend(targets,prop={'size': 30})\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by hydrophobic fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 0.18,0.24, float('inf')]\n",
    "labels = ['less polar','moderate polar','more polar']\n",
    "wholedata_1=wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['composition_polar'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "finalDf_2d_1 =finalDf_2d_1.sample(n = 10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (35,35))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets = ['less polar','moderate polar','more polar']\n",
    "colors = [ '#D73027','#FFFFBF','#ABD9E9']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "ax.legend(targets,prop={'size': 30})\n",
    "ax.grid()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 0, float('inf')]\n",
    "labels = ['negatively charged','positively charged']\n",
    "wholedata_1=wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['charge'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "finalDf_2d_1 =finalDf_2d_1.sample(n = 10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (35,35))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets = [ 'negatively charged','positively charged']\n",
    "colors = [ '#D73027','#ABD9E9']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "ax.legend(targets,prop={'size': 30})\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 2.185384e+04,6.523271e+04, float('inf')]\n",
    "labels = ['small','middle','large']\n",
    "wholedata_1  = wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['mass'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "finalDf_2d_1 =finalDf_2d_1.sample(n = 10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (35,35))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_title('2 component PCA labeled with mass', fontsize = 35)\n",
    "\n",
    "targets = [ 'small','middle','large']\n",
    "colors = [  '#D73027','#FFFFBF','#ABD9E9',]\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=1)\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target), linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "\n",
    "ax.grid()\n",
    "#plt.savefig('./figures/dimension_reduction/PCA_mass.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by the percentage of hydrophobic residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 0.36, 0.43, float('inf')]\n",
    "labels = ['<36%','36-43%','43%']\n",
    "wholedata_1  = wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['composition_hydrophobic'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "finalDf_2d_1 =finalDf_2d_1.sample(n = 100000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (30,30))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_title('2 component PCA labeled with hydrophobic residue percentage', fontsize = 35)\n",
    "\n",
    "targets = [ '<36%','36-43%','43%']\n",
    "colors = [  '#D73027','#FFFFBF','#ABD9E9',]\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.2)\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target), linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "\n",
    "ax.grid()\n",
    "#plt.savefig('./figures/dimension_reduction/PCA_composition_hydrophobic.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by the percentage of proline residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 0.035, 0.062, float('inf')]\n",
    "labels = ['<3.5%','3.5-6.2%','6.2%']\n",
    "wholedata_1  = wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['composition_proline'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "\n",
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (30,30))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_title('2 component PCA labeled with proline residue percentage', fontsize = 35)\n",
    "\n",
    "targets = ['<3.5%','3.5-6.2%','6.2%']\n",
    "colors = [  '#D73027','#FFFFBF','#ABD9E9',]\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target), linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "\n",
    "ax.grid()\n",
    "plt.savefig('./figures/dimension_reduction/PCA_composition_proline.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by the percentage of charged residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [float('-inf'), 0.116, 0.157, float('inf')]\n",
    "labels = ['<11.6%','11.6-15.7%','15.7%']\n",
    "wholedata_1  = wholedata.copy()\n",
    "# Categorize the data\n",
    "wholedata_1['category'] = pd.cut(wholedata['composition_basic'], bins=bins, labels=labels)\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z_1 =wholedata_1.loc[:,['category']]\n",
    "z_1.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_1 = pd.concat([principalDf_2d_1, z_1], axis = 1)\n",
    "\n",
    "#visualize the PCA result on scatter plot\n",
    "fig = plt.figure(figsize = (30,30))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_title('2 component PCA labeled with proline residue percentage', fontsize = 35)\n",
    "\n",
    "targets = ['<3.5%','3.5-6.2%','6.2%']\n",
    "colors = [  '#D73027','#FFFFBF','#ABD9E9',]\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_1['category'] == target)\n",
    "    ax.scatter(finalDf_2d_1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_1.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.5)\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target), linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "\n",
    "ax.grid()\n",
    "plt.savefig('./figures/dimension_reduction/PCA_composition_proline.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the PCA results by the Category from CATH classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata_2 = wholedata.copy()\n",
    "wholedata_2['label_4'] = wholedata_2['label_3'].str[:1]\n",
    "wholedata_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata_2 = wholedata.copy()\n",
    "wholedata_2['label_4'] = wholedata_2['label_3'].str[:1]\n",
    "\n",
    "principalDf_2d_1 = pd.DataFrame(data = principalComponents_1\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "#concatenate the data and label into one dataframe\n",
    "z =wholedata_2.loc[:,['label_4']]\n",
    "z.reset_index(drop=True, inplace=True)\n",
    "\n",
    "finalDf = pd.concat([principalDf_2d_1, z], axis = 1)\n",
    "finalDf.dropna(inplace=True)\n",
    "finalDf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have the finalDf DataFrame with the 'label_4' column\n",
    "\n",
    "fig = plt.figure(figsize=(35, 35))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.set_xlabel('Principal Component 1', fontsize=20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=20)\n",
    "ax.set_title('PCA analysis result labeled with structural category', fontsize=35)\n",
    "#ax.set_xticks(np.arange(-120, 120, 5))\n",
    "#ax.set_yticks(np.arange(-120, 120, 5))\n",
    "\n",
    "targets = ['1','2','3','4','6']#[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
    "colors = ['#4575b4',  '#fee090', '#e0f3f8', '#fc8d59', '#d73027']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = finalDf['label_4'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'],\n",
    "               finalDf.loc[indicesToKeep, 'principal component 2'],\n",
    "               c=color,\n",
    "               s=5,\n",
    "               alpha=0.8)\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "# Create custom legend handles\n",
    "legend_handles = [Patch(facecolor=color, edgecolor=color, label=str(target), linewidth=1, alpha=0.8) for target, color in zip(targets, colors)]\n",
    "ax.legend(handles=legend_handles, prop={'size': 30})\n",
    "#plt.savefig('./figures/dimension_reduction/PCA_CATH_labeling.png',dpi=300)\n",
    "# Add the custom legend to the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 2 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 2 PCA\n",
    "#remove the extreme outliers and conduct pca again\n",
    "#reindexing pdb and drop the identified extreme outliers \n",
    "wholedata_2 = wholedata.copy()\n",
    "wholedata_2.reset_index(drop=True, inplace = True)\n",
    "wholedata_2.drop(outlier_list ,inplace = True)\n",
    "\n",
    "# Separating out the features\n",
    "x_2 = wholedata_2.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y_2 = wholedata_2.loc[:,['label_1','label_2']].values\n",
    "\n",
    "# Standardizing the features\n",
    "x_2 = MinMaxScaler().fit_transform(x_2)\n",
    "pca_2d_2 = PCA(n_components=2)\n",
    "\n",
    "principalComponents_2d_2= pca_2d_2.fit_transform(x_2)\n",
    "\n",
    "principalDf_2d_2 = pd.DataFrame(data = principalComponents_2d_2\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf_2d_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z =wholedata_2.loc[:,['label_1','label_2']]\n",
    "z.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_2 = pd.concat([principalDf_2d_2, z], axis = 1)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (35,35))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA_2', fontsize = 20)\n",
    "\n",
    "targets = [ 'AF2','PDB', 'Pisces']\n",
    "colors = [ '#D5D6BF', '#B87C1B','#6279A2']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_2['label_1'] == target) | (finalDf_2d_2['label_2'] == target)\n",
    "    ax.scatter(finalDf_2d_2.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_2.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.8)\n",
    "ax.legend(targets,prop={'size': 30})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 2 PCA: To find out the ratio of explained variance by each components\n",
    "#PCA result analysis\n",
    "explained_variance_ratio = pca_2d_2.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Step 3: Interpret principal components\n",
    "loadings = pca_2d_2.components_\n",
    "\n",
    "loading_matrix = pd.DataFrame(loadings, columns=features)\n",
    "#print(\"Loading Matrix:\\n\", loading_matrix)\n",
    "loading_matrix.to_csv('./output_tables/2dPCA_1_principal_components_interpretation_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf_2d_1.describe()\n",
    "-6.65e-01\t8.18e-01\n",
    "-7.01-01    7.60-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf_2d_2.describe()\n",
    "-7.05e-01\t7.78e-01\n",
    "-4.75e-01   7.22e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata_new = wholedata.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholedata_new.to_csv('wholedata_pdb_af2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 3 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all outliers\n",
    "#find out all outliers in the 2dpca_1\n",
    "principalDf_2d_1.columns=['principal component 1', 'principal component 2']\n",
    "principalDf_2d_1_3 = principalDf_2d_1.copy()\n",
    "\n",
    "X_3 = principalDf_2d_1_3.iloc[:, 0:2].values\n",
    "\n",
    "\n",
    "pc1_upper = 0.53 \n",
    "pc1_lower = -0.59\n",
    "pc1_all_outliers = (X_1[:, 0] > pc1_upper) | (X_1[:, 0] < pc1_lower)\n",
    "\n",
    "pc2_upper = 0.52 \n",
    "pc2_lower = -0.52 \n",
    "pc2_all_outliers = (X_1[:, 1] > pc2_upper) | (X_1[:, 1] < pc2_lower)\n",
    "\n",
    "all_outliers = pd.concat([principalDf_2d_1_3[pc1_all_outliers], principalDf_2d_1_3[pc2_all_outliers]], axis=0)\n",
    "all_outliers_list = all_outliers.index.tolist()\n",
    "len (all_outliers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 3 PCA\n",
    "#remove the all outliers and conduct pca again\n",
    "wholedata_3 = wholedata_1.copy()\n",
    "wholedata_3.reset_index(drop=True, inplace=True)\n",
    "wholedata_3.drop(all_outliers_list,inplace=True) #pdb_3 = pdb with removed extreme outliers\n",
    "\n",
    "\n",
    "# Separating out the features\n",
    "x_3 = wholedata_3.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y_3 = wholedata_3.loc[:,['label_1','label_2']].values\n",
    "\n",
    "# Standardizing the features\n",
    "x_3 = MinMaxScaler().fit_transform(x_3)\n",
    "pca_2d_3 = PCA(n_components=2)\n",
    "\n",
    "principalComponents_2d_3= pca_2d_3.fit_transform(x_3)\n",
    "\n",
    "principalDf_2d_3 = pd.DataFrame(data = principalComponents_2d_3\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "z_3 =wholedata_3.loc[:,['label_1','label_2']]\n",
    "z_3.reset_index(drop=True, inplace=True)\n",
    "finalDf_2d_3 = pd.concat([principalDf_2d_3, z_3], axis = 1)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (35,35))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA_3', fontsize = 20)\n",
    "\n",
    "targets = ['AF2','PDB',  'Pisces']\n",
    "colors = ['#D5D6BF','#B87C1B',  '#6279A2']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = (finalDf_2d_3['label_1'] == target) | (finalDf_2d_3['label_2'] == target)\n",
    "    ax.scatter(finalDf_2d_3.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_2d_3.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha=0.8)\n",
    "ax.legend(targets,prop={'size': 30})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA result analysis\n",
    "explained_variance_ratio = pca_2d_3.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Step 3: Interpret principal components\n",
    "features = [\n",
    "       'hydrophobic_fitness', 'isoelectric_point', 'charge',\n",
    "       'packing_density',  'evoef2_total',\n",
    "       'evoef2_intraR_total', 'rosetta_total', 'rosetta_fa_intra_rep',\n",
    "       'rosetta_fa_elec', 'rosetta_fa_sol', 'rosetta_lk_ball_wtd',\n",
    "       'rosetta_fa_intra_sol_xover4', 'rosetta_hbond_bb_sc',\n",
    "       'rosetta_hbond_sc', 'rosetta_rama_prepro', 'rosetta_p_aa_pp',\n",
    "       'rosetta_fa_dun', 'rosetta_omega', 'rosetta_pro_close',\n",
    "       'aggrescan3d_avg_value', 'aggrescan3d_max_value']\n",
    "loadings = pca_2d_3.components_\n",
    "loadings\n",
    "#loading_matrix = pd.DataFrame(loadings, columns=features)\n",
    "#print(\"Loading Matrix:\\n\", loading_matrix)\n",
    "loading_matrix.to_csv('2dPCA_3_principal_components_interpretation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
